{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "53d44a9e",
      "metadata": {
        "id": "53d44a9e"
      },
      "source": [
        "# Playing with transformers\n",
        "\n",
        "By [Allison Parrish](http://www.decontextualize.com/)\n",
        "\n",
        "(This is a rough draft!)\n",
        "\n",
        "[Transformers](https://huggingface.co/transformers/) is a Python library released by [Hugging Face](https://huggingface.co/) to make it easy to use pre-trained transformer language models. This notebook takes you through the basics of how to generate text with this library, and demonstrates a few simple techniques you can use to assert finer-grained control over the text generation procedure, like logit warping and fine-tuning.\n",
        "\n",
        "## Warning\n",
        "\n",
        "Before you begin, I recommend reading [On the Dangers of Stochastic Parrots: Can Language Models Be Too Big?](https://dl.acm.org/doi/10.1145/3442188.3445922) by Bender et al. The paper outlines the difficulties and potential for harm in large pre-trained language models, and suggests some techniques for training and making use of these models responsibly. For my part, I have the following recommendations:\n",
        "\n",
        "* Don't use pre-trained language models in an automated context. By \"automated context\" I mean things like web apps, Twitter bots, etc. You just can't guarantee that the output of one of these models won't be harmful. (I might make an exception to this rule if your automated service has *very* robust moderation, but even then, it's iffy.)\n",
        "* Always read through all of your language model outputs before publishing them.\n",
        "* Don't use language models to trick people.\n",
        "\n",
        "## What is a \"transformer\" though\n",
        "\n",
        "\"Transformer\" is a name applied to neural network architectures that make use of a mechanism called [\"attention\"](https://jalammar.github.io/visualizing-neural-machine-translation-mechanics-of-seq2seq-models-with-attention/) and can be trained in parallel (rather than sequentially, as is the case with other neural network architectures that are frequently used to model sequences, like recurrent neural networks). The [introduction of this architecture](https://arxiv.org/abs/1706.03762) initiated a period of tremendous growth in language model capabilities. (Examples of Transformer models include [GPT-2](https://github.com/openai/gpt-2) and [Google T5](https://ai.googleblog.com/2020/02/exploring-transfer-learning-with-t5.html).)\n",
        "\n",
        "This growth is mostly predicated on the fact that the transformer architecture makes it more practical to train language models on larger and larger datasets. As of this writing, state-of-the-art transformer models are often trained on datasets many hundreds of gigabytes in size, and consequently take a tremendous amount of energy (and money, and time) to train. In many cases, it's not practical to train a transformer model from scratch on your own that has the same capabilities. Instead, researchers and artists make use of models that other organizations have trained.\n",
        "\n",
        "## Hugging Face Transformers\n",
        "\n",
        "That's where the Hugging Face Transformers library comes in. It's an easy interface for downloading pre-trained transformer models and making use of them with a consistent API. (For example, we can use the same code to generate text with GPT-2 and [XLNet](https://huggingface.co/transformers/model_doc/xlnet.html)). To install Transformers, you'll first need to install [PyTorch](https://pytorch.org/). If you're running this notebook with Anaconda, you can just run the code in the following cell:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "id": "2d6c019c",
      "metadata": {
        "scrolled": true,
        "id": "2d6c019c",
        "outputId": "4e27b7c2-33ba-4547-a8c1-1e00a3bf1136",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/bin/bash: line 1: conda: command not found\n"
          ]
        }
      ],
      "source": [
        "import sys\n",
        "!conda install --prefix {sys.prefix} -y -c pytorch pytorch"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "51857ed5",
      "metadata": {
        "id": "51857ed5"
      },
      "source": [
        "(Note: You can also use Transformers with Tensorflow, but in practice I've found that PyTorch support in Transformers is better.)\n",
        "\n",
        "Now you can install Transformers by running the following cell:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "id": "b5b3ecda",
      "metadata": {
        "id": "b5b3ecda",
        "outputId": "ef4d400c-f85a-4a81-9ed0-dd04f1793588",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: transformers in /usr/local/lib/python3.10/dist-packages (4.44.2)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers) (3.16.1)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.23.2 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.24.7)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (1.26.4)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers) (24.1)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (6.0.2)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (2024.9.11)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers) (2.32.3)\n",
            "Requirement already satisfied: safetensors>=0.4.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.4.5)\n",
            "Requirement already satisfied: tokenizers<0.20,>=0.19 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.19.1)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers) (4.66.6)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.23.2->transformers) (2024.10.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.23.2->transformers) (4.12.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.4.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2.2.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2024.8.30)\n"
          ]
        }
      ],
      "source": [
        "import sys\n",
        "!{sys.executable} -m pip install transformers"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "d779a381",
      "metadata": {
        "id": "d779a381"
      },
      "source": [
        "## Quick start: load a model and generate text"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ea69e8fd",
      "metadata": {
        "id": "ea69e8fd"
      },
      "source": [
        "I'm going to show you how to load a pre-trained model from the [Hugging Face Models directory](https://huggingface.co/models). First, you need to import the relevant parts of the Transformers library. I'm using the `Auto` classes, which automatically load the correct code based on the model that you choose. The `AutoModelForCausalLM` (that's \"causal\" not \"casual\") is the class you use for text generation tasks (where you want to generate the next word in a sequence)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "id": "c327cc58",
      "metadata": {
        "id": "c327cc58"
      },
      "outputs": [],
      "source": [
        "from transformers import pipeline, AutoModelForCausalLM, AutoTokenizer"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "be708150",
      "metadata": {
        "id": "be708150"
      },
      "source": [
        "In this notebook, I'm going to use [distilgpt2](https://huggingface.co/distilgpt2), a \"distilled\" version of OpenAI's GPT-2 model. The primary benefit of this model is that it is small and fast—it generates text in a speedy fashion even on my old 2013 MacBook Air. You need to load both the model and its associated tokenizer. (We'll talk about the tokenizer in more detail below.) To load, use the `.from_pretrained()` method of the appropriate `Auto` class, like so:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "id": "8cd686cf",
      "metadata": {
        "id": "8cd686cf",
        "outputId": "855e6a3b-a960-44a7-e2b3-220597077a73",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 400,
          "referenced_widgets": [
            "cadbc707d93d43379869adf14d978d07",
            "32fb45ff3a2f4003b0f976d5cd5e7129",
            "1769d23ea3184521a71c40f4d05fdaa7",
            "50b972d771a24c20b0e44b47b9cae97f",
            "65a934267ff64c839b87861e0f173790",
            "c3ee8f678b714794a097033d35406726",
            "04f7d161a7844d249e82beaed6816e7e",
            "198c71e8566c41949e124eb0160ae7d4",
            "1a6aeead7975454a8ce5f4ba5aa18ce1",
            "e2d2b12d4f42496994d8ba6ed8525091",
            "450c6b453eaf46448554b0f357a99746",
            "43617af0c4bc471d8e77049e8450b374",
            "a14161b08eab445388e66df210a1a103",
            "7b90bfd485fb42928598c00c23f4dff6",
            "c4f308054d7f4bb5b895c97feda36e59",
            "e0611492f4f64e6196f3c772f1d8574b",
            "c14e10e2fd304215bb26cde981adb34a",
            "de582300ab6b4620b03bf16aef94138c",
            "159d1ab01183470ba0c7458fe0340ec7",
            "f6791bb357064008b751c9840092d617",
            "1815b6025c9f48db9127a7ac19da9954",
            "cf39e9cfb3c442d584d964fa6569b482",
            "83c1b85be43d4b9d9588272e508d661b",
            "da60b228269f4eda96e758ead0d5bfc7",
            "7fdfe523981f4c10ba6f0f9f57008442",
            "da7020fb8a3d496696386541c2e89ae3",
            "297aa3a8eae74d88b3b7697c54546a2b",
            "3cb9fa97989847eda93d482d33d78e17",
            "45ccf917f9124d66b9a28154a3aebb43",
            "d31ab45b38714fc58ba2a337477e3e6c",
            "7d3ac4ecb9c54fc1a36304aeaa8b2f39",
            "50b515a9bf7b483e865b9eb0e52fd923",
            "2793be2701194e56a792b80c977c9c05",
            "583708bf57f143c6a8b643b7abbc4f28",
            "b51300b3d8ef40e09f9ee10348fa44d5",
            "2f8f062087234989bc1bd50243230835",
            "50db6d96704347669b21a4de873b0335",
            "8afe5892d2a445698573b5a9c900183e",
            "834cdc301eea4168a408a118094d4396",
            "6ff9cfc34ab74f3fbf6cd6aa1ca2c811",
            "5c4e4e4577604ae7880005333a6891d1",
            "8529dc80f202467187129fb6dfb9fb47",
            "94fd597a1e8b4a7bb9012330c6144076",
            "6a4a298e647d4617ad8c009f93809333",
            "d42abe017596440188969f13c405764c",
            "fc87b68665df41908441a3f06784bcd2",
            "256523dfc7f24566bf8ba8f4316ac372",
            "32db5ac14d484349b282723e1eb2d719",
            "8e58da124b794e0e95f5dac4c0e1ff5d",
            "180c11ee0dc04bc09b428b6ecfea1f18",
            "9597b0611daf4bc6907ad41304600330",
            "57b009b604c7467d85fa6d6bd1b94b18",
            "159af0890c0244579e7e992fcc951f7b",
            "a733681a57bf41e184dc02bb287dc9af",
            "3fe33242802b402fa4107b162a00bcbc",
            "40b8c13b9dbc41fd81e255c38dabc06a",
            "f8415178442942689f17c39c9d4e9079",
            "be1bde7f97604894be30ada4e5439176",
            "5ee8886188904061925cf57a47704fa8",
            "2eccb868e7a947f1af4b77e6e712fb86",
            "c1511faef52a4e938857e404c1584ecc",
            "70f18b24d091421f806890bda8d82cca",
            "cb03b2c495c041648d14d95fd48a8757",
            "8e2bb20a9e4a449e9e81f9bc16df177c",
            "e8918272a5f248558a66a2f7c6b569e5",
            "63af51f3d0164a10b345e6d94ceb0c0b",
            "e40c16146e714ec6b2442ea55483195b",
            "1bc87d0420904b628d6f12a95a74292b",
            "8478ed7604294b7293d1a8993b03a567",
            "0307d6a18f414a08a2bac8a98a8fb31a",
            "856dd14b0aac4e0f9ff682cee815bdb7",
            "d7f5cf0d1d87440ab4905adb890f1b15",
            "e3bcf51c7e174ce2976eb71d35d23a13",
            "bb3f7ac192dd4b8c8c829e7f262b330d",
            "b90cb12f006e4dd08329c11b59b1ecad",
            "633d0632a85642a4ae108e078f2987ea",
            "5b23a1f47a7e4838a1d6ac3c65ccd087"
          ]
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/huggingface_hub/utils/_token.py:89: UserWarning: \n",
            "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "tokenizer_config.json:   0%|          | 0.00/26.0 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "cadbc707d93d43379869adf14d978d07"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "config.json:   0%|          | 0.00/762 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "43617af0c4bc471d8e77049e8450b374"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "vocab.json:   0%|          | 0.00/1.04M [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "83c1b85be43d4b9d9588272e508d661b"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "merges.txt:   0%|          | 0.00/456k [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "583708bf57f143c6a8b643b7abbc4f28"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "tokenizer.json:   0%|          | 0.00/1.36M [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "d42abe017596440188969f13c405764c"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "model.safetensors:   0%|          | 0.00/353M [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "40b8c13b9dbc41fd81e255c38dabc06a"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "generation_config.json:   0%|          | 0.00/124 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "e40c16146e714ec6b2442ea55483195b"
            }
          },
          "metadata": {}
        }
      ],
      "source": [
        "tokenizer = AutoTokenizer.from_pretrained('distilgpt2')\n",
        "model = AutoModelForCausalLM.from_pretrained('distilgpt2')"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "91bd8087",
      "metadata": {
        "id": "91bd8087"
      },
      "source": [
        "This might take a little while! The Transformers library downloads the model when you call the `.from_pretrained()` method, and some of the models are very large. (DistilGPT2, at 300MB, is on the small side.) The library will cache these files for later, so you won't need to download them again on the same machine.\n",
        "\n",
        "Once you have the tokenizer and the model, you can create a Transformers *pipeline*. A pipeline groups together and abstracts away the intermediate steps of a machine learning procedure. The Transformers library has [many types of pipeline](https://huggingface.co/transformers/main_classes/pipelines.html), but we're going to create a text generation pipeline, using the model and tokenizer that we just loaded. Here's what it looks like:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "id": "68e0808e",
      "metadata": {
        "id": "68e0808e",
        "outputId": "385951c2-0dd6-4fbe-c5af-f66e94255882",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Hardware accelerator e.g. GPU is available in the environment, but no `device` argument is passed to the `Pipeline` object. Model will be on CPU.\n"
          ]
        }
      ],
      "source": [
        "generator = pipeline('text-generation', model=model, tokenizer=tokenizer)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "d838707a",
      "metadata": {
        "id": "d838707a"
      },
      "source": [
        "Having created this pipeline, we can use it to generate text by calling the pipeline object as though it were a function. The parameter that you pass in is the \"prompt\"—i.e., the text whose completion you want to predict."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "id": "ead3d4a1",
      "metadata": {
        "id": "ead3d4a1",
        "outputId": "622ce558-f025-418a-9d8e-aaaf62130a0b",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[{'generated_text': \"Two roads diverged in a yellow wood, and there was a small creek on the banks. In the event of the fire, there was no further danger.\\n\\n\\n\\nLincoln County Sheriff's Office\\nThis incident took place on May 16\"}]"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ],
      "source": [
        "generator(\"Two roads diverged in a yellow wood, and\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "2852f414",
      "metadata": {
        "id": "2852f414"
      },
      "source": [
        "This call returns a list of dictionaries, where each dictionary has a key `generated_text` whose value is the text that was generated. If you just want the generated string, you can do this:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "id": "16f15e4a",
      "metadata": {
        "id": "16f15e4a",
        "outputId": "aa63d13b-1a4e-4916-cc8a-43f357273fc0",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 70
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'Two roads diverged in a yellow wood, and the water seemed to turn green. The river water still turned into snow. A man came along to rescue him. He was a woman, but without her consent, the man went to bed.\\n\\n'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 7
        }
      ],
      "source": [
        "generator(\"Two roads diverged in a yellow wood, and\")[0]['generated_text']"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "2cd63531",
      "metadata": {
        "id": "2cd63531"
      },
      "source": [
        "There are other parameters that you can pass to the text generation pipeline—basically, you can pass in any of the parameters that you could pass to a causal language model's [.generate() method](https://huggingface.co/transformers/main_classes/model.html#transformers.generation_tf_utils.TFGenerationMixin.generate). We'll go over those parameters in a second, but before we do so, I think it'll be helpful to take a step back and examine what exactly is happening in the generation process."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "0dade853",
      "metadata": {
        "id": "0dade853"
      },
      "source": [
        "## Tokenization\n",
        "\n",
        "Machine learning models don't work on text directly; instead, they operate on numbers that correspond to parts of a text. Breaking a text up into enumerable parts is called *tokenization*. In this class, we've already explored several easy and common forms of tokenization, e.g., breaking a text up into characters, or breaking a text up into words. Most machine learning models now use a form of *sub-word* tokenization, in which a text is broken up into units that don't neatly correspond to either individual characters or whole words. The tokenization procedure itself is derived from statistical properties of the corpus—so tokenizers are, in a sense, \"trained\" in the same way that a machine learning model is. (This is why you have to load the tokenizer, the same way you load a model.)\n",
        "\n",
        "A tokenizer has a *vocabulary*, which is the set of all possible unique tokens that the tokenizer recognizes. You can examine the vocabulary by calling the tokenizer's `.get_vocab()` method:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "id": "97b04c3d",
      "metadata": {
        "id": "97b04c3d"
      },
      "outputs": [],
      "source": [
        "vocab = tokenizer.get_vocab()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "25e1b432",
      "metadata": {
        "id": "25e1b432"
      },
      "source": [
        "Use `len()` to see how many items are in the vocabulary:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "id": "91c47f0b",
      "metadata": {
        "id": "91c47f0b",
        "outputId": "521bbf10-2443-4af7-f615-213892c2c1c2",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "50257"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ],
      "source": [
        "len(vocab)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "379643e5",
      "metadata": {
        "id": "379643e5"
      },
      "source": [
        "The vocabulary is returned as a dictionary that maps tokens to their IDs. Let's just take a peek into what that looks like. I'm going to randomly sample a few items from the dictionary, like so:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "id": "05ca6ef7",
      "metadata": {
        "id": "05ca6ef7",
        "outputId": "f615bab1-59b7-4f2a-a796-16b1fe0b2267",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-10-2b05504ed549>:2: DeprecationWarning: Sampling from a set deprecated\n",
            "since Python 3.9 and will be removed in a subsequent version.\n",
            "  random.sample(vocab.items(), 10)\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('Ġmelting', 24203),\n",
              " ('ĠHighlands', 46411),\n",
              " ('ierra', 16367),\n",
              " ('ĠJavier', 44492),\n",
              " ('Ġ2003', 5816),\n",
              " ('EGIN', 43312),\n",
              " ('Ġcontentious', 27154),\n",
              " ('Ġgenetics', 25862),\n",
              " ('ibus', 26333),\n",
              " ('éĽ', 37239)]"
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ],
      "source": [
        "import random\n",
        "random.sample(vocab.items(), 10)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "08fd6338",
      "metadata": {
        "id": "08fd6338"
      },
      "source": [
        "The results look pretty weird, and there a bunch of things to explain. First off, let's discuss the mysterious `Ġ` character. Subword tokenizers generally don't start off with information about where word boundaries occur; instead, they \"learn\" word boundaries as part of the process of \"training\" the tokenizer. The `Ġ` character is a special character that represents a space. Second, we can see that in many cases, the subword tokenizer does actually end up with tokens in its vocabulary that represent entire words. However, in other cases, we end up with what look like word *parts*. This is by design! Because some tokens represent word parts, the tokenizer can potentially encode *any* word—even words that were not present in the original corpus—by tokenizing that word as a sequence of parts.\n",
        "\n",
        "To demonstrate, let's actually encode a string with the tokenizer using its `.encode()` method. Just pass in a string, and you'll get back a list of IDs:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "id": "c5812cc0",
      "metadata": {
        "id": "c5812cc0",
        "outputId": "bd6984a2-de0a-4c32-cdcf-2ca17b57ef58",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[3856,\n",
              " 2946,\n",
              " 0,\n",
              " 1052,\n",
              " 435,\n",
              " 397,\n",
              " 1603,\n",
              " 281,\n",
              " 368,\n",
              " 505,\n",
              " 13,\n",
              " 1168,\n",
              " 3019,\n",
              " 89,\n",
              " 499,\n",
              " 0]"
            ]
          },
          "metadata": {},
          "execution_count": 11
        }
      ],
      "source": [
        "src = \"Behold! An alabaster anemone. Zzzzap!\"\n",
        "tokenizer.encode(src)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "321708f1",
      "metadata": {
        "id": "321708f1"
      },
      "source": [
        "The tokenizer encodes this string of four words into sixteen tokens. You can find the token corresponding to the ID using the tokenizer's `.decode()` method:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "id": "cd960280",
      "metadata": {
        "id": "cd960280",
        "outputId": "ff1d2d79-7510-46b6-ec5e-b8e8a4a19524",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'aster'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 12
        }
      ],
      "source": [
        "tokenizer.decode(1603)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "89a1eee3",
      "metadata": {
        "id": "89a1eee3"
      },
      "source": [
        "With this, we can see how the tokenizer broke up the original string into units:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "id": "98903ac0",
      "metadata": {
        "id": "98903ac0",
        "outputId": "360cd5e3-d407-4633-9295-00de219606c8",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "3856 → 'Be'\n",
            "2946 → 'hold'\n",
            "0 → '!'\n",
            "1052 → ' An'\n",
            "435 → ' al'\n",
            "397 → 'ab'\n",
            "1603 → 'aster'\n",
            "281 → ' an'\n",
            "368 → 'em'\n",
            "505 → 'one'\n",
            "13 → '.'\n",
            "1168 → ' Z'\n",
            "3019 → 'zz'\n",
            "89 → 'z'\n",
            "499 → 'ap'\n",
            "0 → '!'\n"
          ]
        }
      ],
      "source": [
        "for token_id in tokenizer.encode(src):\n",
        "    print(token_id, \"→\", \"'\" + tokenizer.decode(token_id) + \"'\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "8db8217a",
      "metadata": {
        "id": "8db8217a"
      },
      "source": [
        "(I included quotation marks in this output to emphasize the fact that the text of the token includes whitespace.)\n",
        "\n",
        "You can decode an entire list of IDs using the `.decode()` function as well:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "id": "6f994593",
      "metadata": {
        "id": "6f994593",
        "outputId": "966f0fd5-8300-47db-bbc0-b52fe8914baf",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'Behold! An alabaster anemone. Zzzzap!'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 14
        }
      ],
      "source": [
        "token_ids = tokenizer.encode(src)\n",
        "tokenizer.decode(token_ids)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "618ffe34",
      "metadata": {
        "id": "618ffe34"
      },
      "source": [
        "For fun, get the tokenizer to decode a list of random token IDs:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "id": "35384952",
      "metadata": {
        "id": "35384952",
        "outputId": "8904e2c1-39cb-46ab-b619-2141fa8a56d5",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "' STATE Crist existence …\" surviveTrigger shakyclamationreenswaldthem bishops'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 15
        }
      ],
      "source": [
        "tokenizer.decode(random.sample(list(vocab.values()), 12))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "338d7e58",
      "metadata": {
        "id": "338d7e58"
      },
      "source": [
        "Another way to tokenize a text is to call the tokenizer as though it's a function, passing in a list of strings as an argument:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "id": "adaec157",
      "metadata": {
        "id": "adaec157",
        "outputId": "245fcfc7-703d-430e-dcb9-b40a77043ef5",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'input_ids': tensor([[5661,  318,  257, 1332],\n",
              "        [5661,  318, 1194, 1332]]), 'attention_mask': tensor([[1, 1, 1, 1],\n",
              "        [1, 1, 1, 1]])}"
            ]
          },
          "metadata": {},
          "execution_count": 16
        }
      ],
      "source": [
        "tokenizer([\"this is a test\", \"this is another test\"], return_tensors=\"pt\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c973a65f",
      "metadata": {
        "id": "c973a65f"
      },
      "source": [
        "The value returned here is a dictionary in the format that the model is expecting, if you want to run the model \"by hand\" instead of using a pipeline, which is what we're going to do below. The `return_tensors` parameter directs the tokenizer to return the results as a PyTorch tensor instead of a Python list, which is also a requirement for passing the values directly to the model."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "1c78ce45",
      "metadata": {
        "id": "1c78ce45"
      },
      "source": [
        "## Generation in more detail (advanced, but interesting)\n",
        "\n",
        "So what's actually happening when you ask the model to generate text is this: you encode the prompt as a sequence of IDs using the tokenizer, and then the model assigns a probability to every token in the tokenizer's vocabulary, based on which tokens it thinks are most likely to come next. Here's what it looks like to run that process \"by hand,\" so to speak. First, create the prompt:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "id": "f3c65459",
      "metadata": {
        "id": "f3c65459"
      },
      "outputs": [],
      "source": [
        "prompt = \"Two roads diverged in a yellow wood, and\""
      ]
    },
    {
      "cell_type": "markdown",
      "id": "1eed01b9",
      "metadata": {
        "id": "1eed01b9"
      },
      "source": [
        "Then encode the prompt as a sequence of token IDs:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "id": "3de8d01d",
      "metadata": {
        "id": "3de8d01d"
      },
      "outputs": [],
      "source": [
        "prompt_encoded = tokenizer([prompt], return_tensors=\"pt\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "id": "35b6f910",
      "metadata": {
        "scrolled": true,
        "id": "35b6f910",
        "outputId": "3e6d69c9-57d5-42b0-d60c-242b6bdf364d",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'input_ids': tensor([[ 7571,  9725, 12312,  2004,   287,   257,  7872,  4898,    11,   290]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1]])}"
            ]
          },
          "metadata": {},
          "execution_count": 19
        }
      ],
      "source": [
        "prompt_encoded"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "17d893e3",
      "metadata": {
        "id": "17d893e3"
      },
      "source": [
        "Then we call the model as though it were a function, passing in the key/value pairs that the tokenizer returned as parameters (using [Python's `**` operator](https://docs.python.org/3/tutorial/controlflow.html#unpacking-argument-lists)):"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "id": "7f145c1f",
      "metadata": {
        "id": "7f145c1f"
      },
      "outputs": [],
      "source": [
        "result = model(**prompt_encoded)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "d2e13385",
      "metadata": {
        "id": "d2e13385"
      },
      "source": [
        "The value returned from calling the model is an object with various attributes that we could examine. I'm most interested in `.logits`, which is a PyTorch tensor that contains information about the probability that the model assigned to each vocabulary item. (\"Tensor\" btw is just a fancy word for \"array with a bunch of dimensions.\") The prediction for the next token can be found in the very last row of this tensor:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "id": "67979471",
      "metadata": {
        "id": "67979471",
        "outputId": "d98e0d6a-272d-44d3-ba63-80b99223c6de",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([-75.0658, -73.7585, -76.3894,  ..., -75.8365, -73.6258, -73.6459],\n",
              "       grad_fn=<SelectBackward0>)"
            ]
          },
          "metadata": {},
          "execution_count": 21
        }
      ],
      "source": [
        "next_token_probs = result.logits[0,-1]\n",
        "next_token_probs"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "56d2cfd7",
      "metadata": {
        "id": "56d2cfd7"
      },
      "source": [
        "The scores are shown in \"raw\" form, meaning that they don't have the kinds of values that we would normally associate with a probability distribution (i.e., multiple options all adding up to one). But we can still compare them in this state. Higher numbers mean higher probability.\n",
        "\n",
        "This tensor has a shape that corresponds to the number of vocabulary items:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "id": "7c7770a9",
      "metadata": {
        "id": "7c7770a9",
        "outputId": "b9d9f4f4-2c30-4df7-bafe-becbc6c4f4bb",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([50257])"
            ]
          },
          "metadata": {},
          "execution_count": 22
        }
      ],
      "source": [
        "next_token_probs.shape"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "20efcde3",
      "metadata": {
        "id": "20efcde3"
      },
      "source": [
        "And we can actually inquire about the probability of particular tokens by looking them up. The code in the following cells uses the tokenizer's `.encode()` method to convert a token to its ID, then looks up the ID by index in the array with the predictions. (The `.item()` call converts the resulting PyTorch tensor to a native Python value, which just makes the result a bit easier to look at.)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "id": "a54d3d07",
      "metadata": {
        "id": "a54d3d07",
        "outputId": "0ad190ea-75d2-4f40-f4bd-281487e32e22",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "-63.1175651550293"
            ]
          },
          "metadata": {},
          "execution_count": 23
        }
      ],
      "source": [
        "next_token_probs[tokenizer.encode(' the')].item()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "id": "258b656e",
      "metadata": {
        "id": "258b656e",
        "outputId": "c965cef5-e5b8-447a-a72a-5f2916482700",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "-73.56359100341797"
            ]
          },
          "metadata": {},
          "execution_count": 24
        }
      ],
      "source": [
        "next_token_probs[tokenizer.encode(' x')].item()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "id": "35266957",
      "metadata": {
        "id": "35266957",
        "outputId": "4b457e41-ca70-4da4-a42e-97b9fb73347f",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "-66.07611083984375"
            ]
          },
          "metadata": {},
          "execution_count": 25
        }
      ],
      "source": [
        "next_token_probs[tokenizer.encode(' an')].item()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ce625991",
      "metadata": {
        "id": "ce625991"
      },
      "source": [
        "We can see that the tokens ` the` and ` an` have fairly high probability, while the token ` x` has low probability.  Interesting!\n",
        "\n",
        "### Generating text, the home-grown way\n",
        "\n",
        "Using the PyTorch library, we can get a list of the most likely tokens to come next. (This has some dark magic in it if you're not familiar with PyTorch—or another array processing library like NumPy—so... just trust me for a sec.)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "id": "c0cff0a4",
      "metadata": {
        "id": "c0cff0a4",
        "outputId": "f8f3e0ae-2b9d-4484-ed39-c2af26bf4413",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "' the'\n",
            "' a'\n",
            "' two'\n",
            "' one'\n",
            "' it'\n",
            "' were'\n",
            "' some'\n",
            "' there'\n",
            "' they'\n",
            "' in'\n",
            "' then'\n",
            "' another'\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "for idx in reversed(torch.argsort(next_token_probs)[-12:]):\n",
        "    print(\"'\" + tokenizer.decode(idx) + \"'\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "97c8cc46",
      "metadata": {
        "id": "97c8cc46"
      },
      "source": [
        "(Again, I've added in the quotation marks so you can clearly see that these tokens have whitespace at the beginning.) These are the top twelve tokens to come next in the sequence, as predicted by the model. One way to *generate* a text would be to take one of these tokens—maybe the top-scoring token, maybe one of the top *n* picked at random, append it to our original list of tokens, ask the model to make a prediction on *that* list of tokens, and repeat. The loop would look something like this:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "id": "165247bd",
      "metadata": {
        "id": "165247bd",
        "outputId": "b0da671e-aebd-4b77-d431-bd9085522200",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Two roads diverged in a yellow wood, and the\n",
            "Two roads diverged in a yellow wood, and the car\n",
            "Two roads diverged in a yellow wood, and the car that\n",
            "Two roads diverged in a yellow wood, and the car that crashed\n",
            "Two roads diverged in a yellow wood, and the car that crashed at\n",
            "Two roads diverged in a yellow wood, and the car that crashed at night\n",
            "Two roads diverged in a yellow wood, and the car that crashed at night.\n",
            "Two roads diverged in a yellow wood, and the car that crashed at night.\n",
            "\n",
            "\n",
            "Two roads diverged in a yellow wood, and the car that crashed at night.\n",
            "\n",
            "Police\n",
            "Two roads diverged in a yellow wood, and the car that crashed at night.\n",
            "\n",
            "Police found\n"
          ]
        }
      ],
      "source": [
        "prompt = \"Two roads diverged in a yellow wood, and\"\n",
        "for i in range(10):\n",
        "    # encode the prompt\n",
        "    prompt_encoded = tokenizer([prompt], return_tensors=\"pt\")\n",
        "    # run a forward pass on the network\n",
        "    result = model(**prompt_encoded)\n",
        "    # get the probabilities for the next word\n",
        "    next_token_probs = result.logits[0,-1]\n",
        "    # sort by value, get the top 12 (you can change this number! try 1, or 1000)\n",
        "    nexts = torch.argsort(next_token_probs)[-12:]\n",
        "    # append the decoded ID to the current prompt\n",
        "    prompt += tokenizer.decode(random.choice(nexts))\n",
        "    print(prompt)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "73fb8fb9",
      "metadata": {
        "id": "73fb8fb9"
      },
      "source": [
        "### The `.generate()` method\n",
        "\n",
        "Our home-grown solution above does the job, but it's very rudimentary. Because generating text is such a common use-case, the Transformers library provides a `.generate()` method that is quite fast and also has a bunch of bells and whistles that we can exploit to add expressiveness to our use of the language model. Under the hood, though, the `.generate()` method is essentially doing exactly what we did above—iteratively constructing a string based on predicted tokens from the model. Use the `.generate()` method like this:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "id": "40aea2d3",
      "metadata": {
        "id": "40aea2d3",
        "outputId": "00a04ead-5a10-4ea2-e761-0b66d32f1ed9",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 70
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'Two roads diverged in a yellow wood, and the river\\'s entrance was covered with blue paint.\\n\\n\\n\\nThe town of Lomber has since apologized in a statement.\\n\\n\"In this regrettable, but regrettable manner,'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 28
        }
      ],
      "source": [
        "prompt = \"Two roads diverged in a yellow wood, and\"\n",
        "prompt_encoded = tokenizer(prompt, return_tensors=\"pt\") # the \"return_tensors\" thing is important!\n",
        "result = model.generate(**prompt_encoded)[0]\n",
        "tokenizer.decode(result, skip_special_tokens=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e92aef49",
      "metadata": {
        "id": "e92aef49"
      },
      "source": [
        "For a more detailed overview of all of the ways you can use `.generate()`, see [How to generate](https://huggingface.co/blog/how-to-generate) on the Hugging Face blog. One argument of `.generate()` that is useful right off the bat is `max_length`, which continues the generation process for the number of tokens you specify:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "id": "144cc102",
      "metadata": {
        "id": "144cc102",
        "outputId": "e47e3106-3ea1-4581-e6bc-215763e11ac9",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 105
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'Two roads diverged in a yellow wood, and on top of it was a big sign, with lights coming from outside.\\n\\n\\n\\n\\n\\nIn one of the cars the two cyclists crossed came near the entrance gate and went to the main section of the road.\\nAround the corner, two trucks arrived and stopped for the sign. The other two trucks arrived inside the road, and the police took issue with it.\\nPolice refused to comment on the situation.\\nThe driver of the road drove away just outside the intersection, and there was no police car.\\nThe red pickup truck, which started up the road, was left leaning heavily.\\nThe bus was then pulled over at 7 p.m. to try and get on a high speed.'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 29
        }
      ],
      "source": [
        "prompt = \"Two roads diverged in a yellow wood, and\"\n",
        "prompt_encoded = tokenizer(prompt, return_tensors=\"pt\")\n",
        "result = model.generate(**prompt_encoded, max_length=250)[0]\n",
        "tokenizer.decode(result, skip_special_tokens=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ea940807",
      "metadata": {
        "id": "ea940807"
      },
      "source": [
        "### Back to the pipeline\n",
        "\n",
        "The process of encoding the prompt and decoding the results is pretty tedious. That's why the \"pipeline\" was invented. The `text-generation` pipeline takes care of encoding and decoding for you. Create a pipeline by calling `pipeline` with `text-generation` as the first parameter, and then the model and tokenizer that you want to use:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "id": "5b73ec86",
      "metadata": {
        "id": "5b73ec86",
        "outputId": "b7fb1578-37bd-4d98-98e7-d7241ba09b86",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Hardware accelerator e.g. GPU is available in the environment, but no `device` argument is passed to the `Pipeline` object. Model will be on CPU.\n"
          ]
        }
      ],
      "source": [
        "generator = pipeline('text-generation', model=model, tokenizer=tokenizer)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "bbb3539f",
      "metadata": {
        "id": "bbb3539f"
      },
      "source": [
        "And then you can generate text with the pipeline. The first argument is the prompt; any remaining parameters will be forwarded to the model's `.generate()` method."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "id": "f7c8e6a2",
      "metadata": {
        "scrolled": true,
        "id": "f7c8e6a2",
        "outputId": "6124ae02-f0ea-44c6-d73d-60fbc64e757e",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[{'generated_text': 'Two roads diverged in a yellow van and were stopped in the middle of traffic.\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nTURNING\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n'}]"
            ]
          },
          "metadata": {},
          "execution_count": 31
        }
      ],
      "source": [
        "generator(\"Two roads diverged in a yellow\",\n",
        "          max_length=100)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e5f4cc74",
      "metadata": {
        "id": "e5f4cc74"
      },
      "source": [
        "As a reminder, to get the actual generated text, use indexing to get the value of the dictionary in the list returned from the pipeline:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "id": "93ef297e",
      "metadata": {
        "id": "93ef297e",
        "outputId": "8f5a5e7b-e583-46f7-a2b1-928e078740dc",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 87
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "\"Two roads diverged in a yellow Volkswagen Beetle into a river, causing a crash that killed 34 people and forced authorities to close off the area.\\n\\n\\n\\nThe accident happened over Christmas in Lake Michigan along Route 1west.\\nAuthorities said the VW Beetle struck two vehicles on the way to the Lakeland River, a few minutes before crossing the embankment.\\nAuthorities said a friend of the victim said the victim was wearing a backpack and was wearing a black dress. He didn't know\""
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 32
        }
      ],
      "source": [
        "generator(\"Two roads diverged in a yellow\",\n",
        "          max_length=100)[0]['generated_text']"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "6e84bb46",
      "metadata": {
        "id": "6e84bb46"
      },
      "source": [
        "## Controlling the model\n",
        "\n",
        "By default, the `distilgpt2` model samples from the possible next tokens, weighted by the probability assigned to that token. This strategy leads to text that shows a good deal of variety, but there are strategies that we can use and parameters that we can tweak to exert a little more control over the model's output. In this section, I show a few of these strategies.\n",
        "\n",
        "### The magic of the prompt\n",
        "\n",
        "Transformer models are often able to follow up on cues you give about the desired content and style of the text in the prompt itself. The smaller transformer models aren't especially good at this, but it's still worth playing around with. For example, to get `distilgpt2` to generate something that looks like a movie review:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "id": "9f51c882",
      "metadata": {
        "id": "9f51c882",
        "outputId": "38c6f97c-ba2e-4054-d3e6-9b6424fe6c0a",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "My review of The Road Not Taken, the Movie: A Story of the Journey. It was a great story about a young girl who, in her teens, had to be killed at a crossroads between New York City and Montreal. She was about to kill herself. I mean, it's a really good story. She had an accident and a drug addiction, and she probably was going to die because of that. I mean, I'm not saying you do better than me in The Road Not\n"
          ]
        }
      ],
      "source": [
        "print(generator(\"My review of The Road Not Taken, the Movie:\", max_length=100)[0]['generated_text'])"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "d643be14",
      "metadata": {
        "id": "d643be14"
      },
      "source": [
        "You can also generate dialogues and interview transcripts:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "id": "0295ee5c",
      "metadata": {
        "id": "0295ee5c",
        "outputId": "04bee3dc-31a5-427b-d5b0-971e9adbc57d",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Allison: I took the road less traveled by.\n",
            "Robert Frost: I walked, not in my bed, but I walked, not in my bed, but I walked, not in my bed. And you went. I went. I took the road less traveled by.\n",
            "Robert Frost: I walked with my arm on my shoulder and I walked. I was standing in the doorway of my bathroom. But you walk, I walk, not in my bed. I walked. I walked.\n"
          ]
        }
      ],
      "source": [
        "print(generator(\"Allison: I took the road less traveled by.\\nRobert Frost:\",\n",
        "                max_length=100)[0]['generated_text'])"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "06de3603",
      "metadata": {
        "id": "06de3603"
      },
      "source": [
        "Poetry facts:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 35,
      "id": "cfc9a4c0",
      "metadata": {
        "id": "cfc9a4c0",
        "outputId": "dd4eaef2-26dc-4e49-b456-709cfed41bb3",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "My favorite facts about poetry:\n",
            "\n",
            "1. The American Dream is only going to end\n",
            "I think there are a couple of good things to talk about: The American Dream is not a \"fantasy\" and I think it's something we take seriously.\n",
            "2. It is a time of great human change\n",
            "It's a time of great human change and we've got it all started before us, and I do think as long as we're doing something like this, we won't be\n"
          ]
        }
      ],
      "source": [
        "print(generator(\"My favorite facts about poetry:\\n\\n1.\",\n",
        "                max_length=100)[0]['generated_text'])"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "2e80b074",
      "metadata": {
        "id": "2e80b074"
      },
      "source": [
        "In general, this kind of prompting works best with texts that are likely to have a lot of representation in the training corpus."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "81363f7a",
      "metadata": {
        "id": "81363f7a"
      },
      "source": [
        "### Sampling with temperature\n",
        "\n",
        "As mentioned above, the `distilgpt2` model, by default, picks the next token at random, weighted by the probability that the model assigns to the word. To demonstrate how this works, let's imagine that the model only has five tokens in its vocabulary (instead of 50,000+). A schematic illustration of those probabilities might look like this:\n",
        "\n",
        "    prompt: Whose woods these are I think I\n",
        "    probabilities:\n",
        "        know -> 0.5\n",
        "        knew -> 0.2\n",
        "        smell -> 0.15\n",
        "        see -> 0.1\n",
        "        am -> 0.05\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "15474aa6",
      "metadata": {
        "id": "15474aa6"
      },
      "source": [
        "The probabilities will add up to `1.0`. A probability of `0.5` indicates that the token has a 50% probability of coming next; a probability of 0.2 means that the token has a 20% probability of coming next, etc. Here are those probabilities represented in Python as two lists—one with the words, and one with the probabilities that correspond to those words by index:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 36,
      "id": "67854938",
      "metadata": {
        "id": "67854938"
      },
      "outputs": [],
      "source": [
        "tokens = ['know', 'knew', 'smell', 'see', 'am']\n",
        "probs = [0.5, 0.2, 0.15, 0.1, 0.05]"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f95c6404",
      "metadata": {
        "id": "f95c6404"
      },
      "source": [
        "By default, to select the next token, the generation code picks from this list weighted by probability. The code to do this with PyTorch looks like this:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 37,
      "id": "b9f85011",
      "metadata": {
        "id": "b9f85011",
        "outputId": "6a8060dc-ec2f-464c-a6f1-105586059792",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "know\n"
          ]
        }
      ],
      "source": [
        "index = torch.multinomial(torch.tensor(probs), 1).item()\n",
        "print(tokens[index])"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "636e2d8c",
      "metadata": {
        "id": "636e2d8c"
      },
      "source": [
        "You don't have to worry about the specifics of this code—I'm just using it to demonstrate how the sampling process works. Run the code a few times and you'll see that about half the time you get \"knew\"—the token with the highest probability. Running the code in a loop makes this a bit easier to see:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 38,
      "id": "af732dd1",
      "metadata": {
        "id": "af732dd1",
        "outputId": "23bc9241-0af8-4610-d28f-57a0113d243a",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "know\n",
            "know\n",
            "know\n",
            "know\n",
            "know\n",
            "knew\n",
            "know\n",
            "knew\n",
            "know\n",
            "know\n"
          ]
        }
      ],
      "source": [
        "for i in range(10):\n",
        "    index = torch.multinomial(torch.tensor(probs), 1).item()\n",
        "    print(tokens[index])"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "d12d2fcf",
      "metadata": {
        "id": "d12d2fcf"
      },
      "source": [
        "The generation process has a parameter called *temperature*, which lets you shift the probability distribution of the next token before it's sampled. If the temperature parameter is `1.0`, then sampling will proceed as normal, with the tokens weighted by their estimated probability. If the temperature parameter is less than `1.0`, then tokens that were already probable will get *more* probable. If the temperature parameter is greater than `1.0`, then the probabilities start to even out, approaching a uniform distribution (meaning that no token is more likely to be chosen than any other). To demonstrate this, I've written some code below that applies temperature to the probabilities defined above, and shows the resulting changes:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 39,
      "id": "2fa5775f",
      "metadata": {
        "id": "2fa5775f",
        "outputId": "16824065-2b15-4417-8b3f-49595489637a",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "temperature 0.10\n",
            "know   → 1.00\n",
            "knew   → 0.00\n",
            "smell  → 0.00\n",
            "see    → 0.00\n",
            "am     → 0.00\n",
            "\n",
            "temperature 0.35\n",
            "know   → 0.90\n",
            "knew   → 0.07\n",
            "smell  → 0.03\n",
            "see    → 0.01\n",
            "am     → 0.00\n",
            "\n",
            "temperature 1.00\n",
            "know   → 0.50\n",
            "knew   → 0.20\n",
            "smell  → 0.15\n",
            "see    → 0.10\n",
            "am     → 0.05\n",
            "\n",
            "temperature 2.00\n",
            "know   → 0.34\n",
            "knew   → 0.21\n",
            "smell  → 0.19\n",
            "see    → 0.15\n",
            "am     → 0.11\n",
            "\n",
            "temperature 50.00\n",
            "know   → 0.20\n",
            "knew   → 0.20\n",
            "smell  → 0.20\n",
            "see    → 0.20\n",
            "am     → 0.20\n",
            "\n"
          ]
        }
      ],
      "source": [
        "for temperature in [0.1, 0.35, 1.0, 2.0, 50.0]:\n",
        "    modified = torch.softmax(\n",
        "        torch.log(torch.tensor(probs)) / temperature, dim=-1)\n",
        "    print(f\"temperature {temperature:0.02f}\")\n",
        "    for tok, prob in zip(tokens, modified):\n",
        "        print(tok.ljust(6), \"→\", f\"{prob:0.002f}\")\n",
        "    print()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "7c095c57",
      "metadata": {
        "id": "7c095c57"
      },
      "source": [
        "You can see that at temperature `1.0`, the probabilities are identical to the original. At temperature `0.35`, the probability of the most likely token has been boosted, but the other tokens still have a small chance of occurring. At temperature `0.1`, only the most likely token has a chance of being selected. At temperature `2.0`, the most likely token is still the most likely, but the probabilities of the other tokens have been boosted in comparison; at temperature `50.0`, no token is considered to be more likely than any other.\n",
        "\n",
        "To apply temperature sampling to the model when generating text, pass the `temperature` parameter to the pipeline, like so:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 40,
      "id": "13c5bccd",
      "metadata": {
        "id": "13c5bccd",
        "outputId": "8baf02be-2ec7-41e0-8a10-541241cc7509",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 70
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'Two roads diverged in a yellow light, and the police were called to the scene.\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 40
        }
      ],
      "source": [
        "generator(\"Two roads diverged in a yellow\",\n",
        "          temperature=0.1,\n",
        "          max_length=100)[0]['generated_text']"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "2b94bd84",
      "metadata": {
        "id": "2b94bd84"
      },
      "source": [
        "Low temperatures generally produce predictable, repetitive results. Here's an attempt with high temperature:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 41,
      "id": "93298377",
      "metadata": {
        "id": "93298377",
        "outputId": "b5f587c1-2b97-4efc-ba0a-3e6aea08f40e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 87
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "\"Two roads diverged in a yellow-spouting pattern in front of a parked parked motorcycle in central London as an international bus drove along along one lane with police over at least 23 metres across the River Thames when the suspect stopped and arrested Mr Rother's accomplices just moments late a previous morning at 8.28 AM local times just westwagery, which the police have called unsafe due largely not even noticing from a nearby road user on one road between Lubeldra station at Kingmans\""
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 41
        }
      ],
      "source": [
        "generator(\"Two roads diverged in a yellow\",\n",
        "          temperature=4.0,\n",
        "          max_length=100)[0]['generated_text']"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "61a379f7",
      "metadata": {
        "id": "61a379f7"
      },
      "source": [
        "The higher temperature example produces less likely sequences of words, so the text is a bit livelier—sometimes at the cost of coherence.\n",
        "\n",
        "Adjusting the temperature can be useful when you want the text to be more or less \"weird.\" It can be helpful to adjust the temperature downward when you feel as though the model is producing text that is a bit too unpredictable; it can be helpful to adjust upward when you want to model to take more unexpected turns when generating."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "22b2500c",
      "metadata": {
        "id": "22b2500c"
      },
      "source": [
        "### Top-k sampling\n",
        "\n",
        "By default, the generation process only selects from the top 50 most probable tokens at each step. This is called \"top-k filtering.\" Because of top-k filtering, you're not likely to sample truly unusual tokens even when the temperature is high. You can adjust the threshold for top-k filtering with the `top_k` parameter of the model. For example, adjusting `top_k` to the number of items in the vocabulary ensures that every token gets its chance:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 42,
      "id": "bf4d73cb",
      "metadata": {
        "id": "bf4d73cb",
        "outputId": "1422ac90-3037-4c63-bc9f-b650c4bb4a71",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 87
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'Two roads diverged in a yellow liquid along the northern Huang River in northwest China river. The river in southern Huang River was closed due to a flood, but became advisory on the start of Sunday morning.\\n\\nA girl was killed and eight people were hurt in a sweep on Friday night in Henan province, officials said.\\nWhen officials arrived at the area, a flood began at the scene. Water supplies were scarce after many water gauge losses to previous floods in that cycle. Water has been'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 42
        }
      ],
      "source": [
        "generator(\"Two roads diverged in a yellow\",\n",
        "          top_k=tokenizer.vocab_size,\n",
        "          max_length=100)[0]['generated_text']"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "1fa7b265",
      "metadata": {
        "id": "1fa7b265"
      },
      "source": [
        "Using this with a temperature greater than `1.0` can yield some unusual turns of phrase:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 43,
      "id": "0b690453",
      "metadata": {
        "id": "0b690453",
        "outputId": "80d2e572-8c31-44c5-d9bb-da1cf8170978",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 87
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "\"Two roads diverged in a yellow line for 444 metreswithin the industrial dockyard on Sunday.] It only took 200 min after extensive commensurate object movements had occurred, confirmed Mr District Inquiry Commissioner O Fists MP Jaime Da earlier this week, following an assessment issued by Hariri Division eminent domain lawyers. Prosecutors allege the congestion caused by the sector's congested lanes sealed by noise usually rankled traffic and having caused traffic accidents. Ammunition WAS introduced in 2001 but quickly smuggled into booming London's London airport\""
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 43
        }
      ],
      "source": [
        "generator(\"Two roads diverged in a yellow\",\n",
        "          top_k=tokenizer.vocab_size,\n",
        "          temperature=1.2,\n",
        "          max_length=100)[0]['generated_text']"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "1843eaef",
      "metadata": {
        "id": "1843eaef"
      },
      "source": [
        "On the other extreme, setting the `top_k` value to `1` ensures that *only* the most likely token is chosen at each step. This is the same thing as [\"greedy decoding\"](https://huggingface.co/transformers/main_classes/model.html#transformers.generation_utils.GenerationMixin.greedy_search):"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 44,
      "id": "ef3b18b7",
      "metadata": {
        "id": "ef3b18b7",
        "outputId": "16314791-6281-4bc8-f8eb-8fc3601e84d9",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 70
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'Two roads diverged in a yellow light, and the police were called to the scene.\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 44
        }
      ],
      "source": [
        "generator(\"Two roads diverged in a yellow\",\n",
        "          top_k=1,\n",
        "          max_length=100)[0]['generated_text']"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "4e343fed",
      "metadata": {
        "id": "4e343fed"
      },
      "source": [
        "Playing around with `top_k` and `temperature` in tandem is a good way to make adjustments to the texture of your generated text."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "3d87511e",
      "metadata": {
        "id": "3d87511e"
      },
      "source": [
        "### Logit warping: Exclude \"bad\" words\n",
        "\n",
        "The `.generate()` method has a parameter called `bad_words_ids`, which causes the model to zero out the probabilities of tokens associated with words that you pass in. The intended use of this feature is to stop the model from generating offensive or harmful words. But we can also repurpose it for poetic purposes. For example, in the cell below, I make the model complete the prompt \"It was a dark and stormy\" *without* using the words \"night\" or \"day\":"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 45,
      "id": "e691ce3e",
      "metadata": {
        "id": "e691ce3e",
        "outputId": "a94db9ba-bca9-49bf-9fc5-1634b1285710",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 70
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'It was a dark and stormy hour with the air crashing against our wall, a bright green flag being given to the victims, and I had to get the whole thing together and get the whole thing done.\\n\\nCapella Police have been'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 45
        }
      ],
      "source": [
        "generator(\"It was a dark and stormy\",\n",
        "          bad_words_ids=tokenizer([\" night\", \" day\"]).input_ids)[0]['generated_text']"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "4cdc646c",
      "metadata": {
        "id": "4cdc646c"
      },
      "source": [
        "The syntax for specifying the \"bad words\" is to call the tokenizer on a list of words that you want to exclude, and then get the `.input_ids` attribute of the value returned from calling the tokenizer. This yields a list of lists that looks like this:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 46,
      "id": "8a18221f",
      "metadata": {
        "id": "8a18221f",
        "outputId": "b3988377-0a3d-48b8-e1d9-928e3f4f40fa",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[[3237, 1653], [47, 3258, 680]]"
            ]
          },
          "metadata": {},
          "execution_count": 46
        }
      ],
      "source": [
        "tokenizer([\"Allison\", \"Parrish\"]).input_ids"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e44e2879",
      "metadata": {
        "id": "e44e2879"
      },
      "source": [
        "Note that I used ` night` and ` day` as the words, with leading spaces—this is necessary because I ended the prompt without whitespace, so the model is likely to generate a token with leading whitespace at the next step. I've found that the `bad_words_ids` parameter works best if your list of words includes versions both with and without whitespace.\n",
        "\n",
        "Here's another example: getting the model to complete a prompt without using any forms of the verb *to be*:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 47,
      "id": "a5c7fc50",
      "metadata": {
        "id": "a5c7fc50",
        "outputId": "86b3b822-448a-4beb-eed3-e32cf649b584",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 87
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "\"Once upon a time, they would have noticed the change in conditions of these two worlds, both physically and mentally.\\n\\nThe main protagonist, Kamii Ushimura, has moved into the new world of Aizor's World. From a local perspective, the world of Saka's World, to the full extent of Saka's world, remains a dark and desolate maze of ruins. With no escape routes to the place, the streets and other buildings which once sat where the world originally\""
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 47
        }
      ],
      "source": [
        "generator(\"Once upon a time,\",\n",
        "          bad_words_ids=tokenizer(\n",
        "              [\"be\", \" be\",\n",
        "               \"am\", \" am\",\n",
        "               \"are\", \" are\",\n",
        "               \"is\", \" is\",\n",
        "               \"was\", \" was\",\n",
        "               \"were\", \" were\"]).input_ids,\n",
        "          max_length=100)[0]['generated_text']"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "bec6a2df",
      "metadata": {
        "id": "bec6a2df"
      },
      "source": [
        "You can also create a list of token IDs that you want to exclude on the fly. In the following example, I make a list of token IDs that have the letter `e` in them, and pass that list to the `bad_words_ids` parameter:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 48,
      "id": "9dd76bc3",
      "metadata": {
        "scrolled": true,
        "id": "9dd76bc3",
        "outputId": "83e8b4f6-ad73-4a96-dc4b-d1cb813146be",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Last month, I‣····.‣· ················· ·····································································\n"
          ]
        }
      ],
      "source": [
        "forbidden_ids = []\n",
        "for key, val in tokenizer.get_vocab().items():\n",
        "    if 'e' in key:\n",
        "        forbidden_ids.append([val]) # needs to be a list of lists\n",
        "print(generator(\"Last month, I\",\n",
        "          bad_words_ids=forbidden_ids,\n",
        "          max_length=100)[0]['generated_text'])"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "5db6369e",
      "metadata": {
        "id": "5db6369e"
      },
      "source": [
        "### Fine-tuning a model\n",
        "\n",
        "\"Fine-tuning\" is a way of slightly modifying a model by training it a few extra steps on a corpus of your choice. This process adjusts the probabilities of the model so that it more closely reflects the probabilities of the source text you train it on. Fine-tuning models with Transformers is a little bit tricky! First, you'll need to install Hugging Face's `datasets` package:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 55,
      "id": "66e3757c",
      "metadata": {
        "id": "66e3757c",
        "outputId": "2eb8fb0e-94d1-4173-c87b-81f048984868",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: datasets in /usr/local/lib/python3.10/dist-packages (3.1.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from datasets) (3.16.1)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from datasets) (1.26.4)\n",
            "Requirement already satisfied: pyarrow>=15.0.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (17.0.0)\n",
            "Requirement already satisfied: dill<0.3.9,>=0.3.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (0.3.8)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from datasets) (2.2.2)\n",
            "Requirement already satisfied: requests>=2.32.2 in /usr/local/lib/python3.10/dist-packages (from datasets) (2.32.3)\n",
            "Requirement already satisfied: tqdm>=4.66.3 in /usr/local/lib/python3.10/dist-packages (from datasets) (4.66.6)\n",
            "Requirement already satisfied: xxhash in /usr/local/lib/python3.10/dist-packages (from datasets) (3.5.0)\n",
            "Requirement already satisfied: multiprocess<0.70.17 in /usr/local/lib/python3.10/dist-packages (from datasets) (0.70.16)\n",
            "Requirement already satisfied: fsspec<=2024.9.0,>=2023.1.0 in /usr/local/lib/python3.10/dist-packages (from fsspec[http]<=2024.9.0,>=2023.1.0->datasets) (2024.9.0)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.10/dist-packages (from datasets) (3.10.10)\n",
            "Requirement already satisfied: huggingface-hub>=0.23.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (0.24.7)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from datasets) (24.1)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from datasets) (6.0.2)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (2.4.3)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.3.1)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (24.2.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.5.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (6.1.0)\n",
            "Requirement already satisfied: yarl<2.0,>=1.12.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.17.1)\n",
            "Requirement already satisfied: async-timeout<5.0,>=4.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (4.0.3)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.23.0->datasets) (4.12.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.32.2->datasets) (3.4.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.32.2->datasets) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.32.2->datasets) (2.2.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.32.2->datasets) (2024.8.30)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2024.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2024.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.16.0)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.10/dist-packages (from yarl<2.0,>=1.12.0->aiohttp->datasets) (0.2.0)\n"
          ]
        }
      ],
      "source": [
        "import sys\n",
        "!{sys.executable} -m pip install datasets"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "46e0395a",
      "metadata": {
        "id": "46e0395a"
      },
      "source": [
        "And then import it:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 56,
      "id": "6f4d68e2",
      "metadata": {
        "id": "6f4d68e2"
      },
      "outputs": [],
      "source": [
        "import datasets"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "3ab81720",
      "metadata": {
        "id": "3ab81720"
      },
      "source": [
        "You'll want to select a text file to fine-tune the model on. Fine-tuning works best on large amounts of text, but fine-tuning is also very slow if you're not using a GPU. For demonstration purposes, I create a special version of [Frankenstein](https://www.gutenberg.org/ebooks/84) that contains only the first 20000 characters, and save it to a file:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 57,
      "id": "35a2a3c1",
      "metadata": {
        "id": "35a2a3c1",
        "outputId": "10faae3a-44ef-4912-bce9-4b6d46a7a7c1",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 159
        }
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "FileNotFoundError",
          "evalue": "[Errno 2] No such file or directory: '84-0.txt'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-57-4a95921b7901>\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"84-0-20k.txt\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"w\"\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mfh\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m     \u001b[0mfh\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwrite\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"84-0.txt\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;36m20000\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '84-0.txt'"
          ]
        }
      ],
      "source": [
        "with open(\"./84-0-20k.txt\", \"w\") as fh:\n",
        "    fh.write(open(\"84-0.txt\").read()[:20000])"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "1221ac91",
      "metadata": {
        "id": "1221ac91"
      },
      "source": [
        "Then I load this text file as my fine-tuning dataset:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9a037218",
      "metadata": {
        "id": "9a037218"
      },
      "outputs": [],
      "source": [
        "training_data = datasets.load_dataset('text', data_files=\"84-0-20k.txt\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b31f0ead",
      "metadata": {
        "id": "b31f0ead"
      },
      "source": [
        "Now, there's a bunch of obligatory processing that we need to do to the data in order to prepare it for the model. This is boilerplate stuff, which I'm not going to go into in detail. If you want details, consult Hugging Face's [fine-tuning language models notebook](https://github.com/huggingface/notebooks/blob/master/examples/language_modeling.ipynb).\n",
        "\n",
        "First, we tokenize the text:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "dc75a5ff",
      "metadata": {
        "id": "dc75a5ff"
      },
      "outputs": [],
      "source": [
        "tokenizer.pad_token = tokenizer.eos_token\n",
        "tokenized_training_data = training_data.map(\n",
        "    lambda x: tokenizer(x['text']),\n",
        "    remove_columns=[\"text\"]\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ea41f005",
      "metadata": {
        "id": "ea41f005"
      },
      "source": [
        "Then we break the tokenized text up into batches of tokens:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "595b9bf0",
      "metadata": {
        "id": "595b9bf0"
      },
      "outputs": [],
      "source": [
        "block_size = 64\n",
        "# magic from https://github.com/huggingface/notebooks/blob/master/examples/language_modeling.ipynb\n",
        "def group_texts(examples):\n",
        "    concatenated_examples = {k: sum(examples[k], []) for k in examples.keys()}\n",
        "    total_length = len(concatenated_examples[list(examples.keys())[0]])\n",
        "    total_length = (total_length // block_size) * block_size\n",
        "    result = {\n",
        "        k: [t[i : i + block_size] for i in range(0, total_length, block_size)]\n",
        "        for k, t in concatenated_examples.items()\n",
        "    }\n",
        "    result[\"labels\"] = result[\"input_ids\"].copy()\n",
        "    return result\n",
        "lm_training_data = tokenized_training_data.map(\n",
        "    group_texts,\n",
        "    batched=True,\n",
        "    batch_size=200\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "5e522074",
      "metadata": {
        "id": "5e522074"
      },
      "source": [
        "Now we import the `Trainer` class, which implements a training loop."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "eee70cb2",
      "metadata": {
        "id": "eee70cb2"
      },
      "outputs": [],
      "source": [
        "from transformers import Trainer, TrainingArguments"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a12058c0",
      "metadata": {
        "id": "a12058c0"
      },
      "source": [
        "Running the following cell creates the `Trainer` object. The `output_dir` parameter specifies a directory where your fine-tuned model will be saved. The `num_train_epochs` sets how many \"epochs\" the trainer will run; one epoch is one iteration over the entire dataset. More epochs is better, but even one epoch can significantly change the way the model generates text."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "6d5359f1",
      "metadata": {
        "id": "6d5359f1"
      },
      "outputs": [],
      "source": [
        "trainer = Trainer(model=model,\n",
        "                  train_dataset=lm_training_data['train'],\n",
        "                  args=TrainingArguments(\n",
        "                      output_dir='distilgpt2-finetune-frankenstein20k',\n",
        "                      num_train_epochs=1,\n",
        "                      do_train=True,\n",
        "                      do_eval=False\n",
        "                  ),\n",
        "                  tokenizer=tokenizer)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "22076634",
      "metadata": {
        "id": "22076634"
      },
      "source": [
        "Finally, the cell below will start the training process. If you're running this on a computer without a GPU, it will take a while. You can open this notebook on [Google Colab](http://colab.research.google.com/) if you want and take advantage of the free GPU that Google lets you use."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a0a974bb",
      "metadata": {
        "scrolled": false,
        "id": "a0a974bb"
      },
      "outputs": [],
      "source": [
        "trainer.train()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "bda411f7",
      "metadata": {
        "id": "bda411f7"
      },
      "source": [
        "Running the cell below will save the model to disk:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f18e00fe",
      "metadata": {
        "id": "f18e00fe"
      },
      "outputs": [],
      "source": [
        "trainer.save_model()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "474a59af",
      "metadata": {
        "id": "474a59af"
      },
      "source": [
        "Now you can generate with the fine-tuned model! The fine-tuning process modifies the model in-place, so the `pipeline` you created before will make use of the fine-tuned model. (Note that if you want to get the original `distilgpt2` back, you'll need to reload it with the `.from_pretrained()` method, as demonstrated at the top of the notebook.)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "6f7769a0",
      "metadata": {
        "id": "6f7769a0"
      },
      "outputs": [],
      "source": [
        "generator(\"Two roads diverged in a yellow\", max_length=100)[0]['generated_text']"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "798333f5",
      "metadata": {
        "id": "798333f5"
      },
      "source": [
        "You can see that fine-tuning on even a small dataset produces big changes in the model.\n",
        "\n",
        "If you want to use your fine-tuned model in another project, use the same syntax that we used above to load `distilgpt2`—just replace `distilgpt2` with the name of the directory where you saved your model:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ed9960c9",
      "metadata": {
        "id": "ed9960c9"
      },
      "outputs": [],
      "source": [
        "my_tokenizer = AutoTokenizer.from_pretrained('distilgpt2-finetune-frankenstein20k')\n",
        "my_model = AutoModelForCausalLM.from_pretrained('distilgpt2-finetune-frankenstein20k')"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "4c62d434",
      "metadata": {
        "id": "4c62d434"
      },
      "source": [
        "Now generate with it:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "201ab0c9",
      "metadata": {
        "id": "201ab0c9"
      },
      "outputs": [],
      "source": [
        "my_generator = pipeline(\"text-generation\", model=my_model, tokenizer=my_tokenizer)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "8de43360",
      "metadata": {
        "id": "8de43360"
      },
      "outputs": [],
      "source": [
        "my_generator(\"Two roads diverged in a yellow\")[0]['generated_text']"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.10"
    },
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "accelerator": "GPU",
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "cadbc707d93d43379869adf14d978d07": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_32fb45ff3a2f4003b0f976d5cd5e7129",
              "IPY_MODEL_1769d23ea3184521a71c40f4d05fdaa7",
              "IPY_MODEL_50b972d771a24c20b0e44b47b9cae97f"
            ],
            "layout": "IPY_MODEL_65a934267ff64c839b87861e0f173790"
          }
        },
        "32fb45ff3a2f4003b0f976d5cd5e7129": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_c3ee8f678b714794a097033d35406726",
            "placeholder": "​",
            "style": "IPY_MODEL_04f7d161a7844d249e82beaed6816e7e",
            "value": "tokenizer_config.json: 100%"
          }
        },
        "1769d23ea3184521a71c40f4d05fdaa7": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_198c71e8566c41949e124eb0160ae7d4",
            "max": 26,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_1a6aeead7975454a8ce5f4ba5aa18ce1",
            "value": 26
          }
        },
        "50b972d771a24c20b0e44b47b9cae97f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_e2d2b12d4f42496994d8ba6ed8525091",
            "placeholder": "​",
            "style": "IPY_MODEL_450c6b453eaf46448554b0f357a99746",
            "value": " 26.0/26.0 [00:00&lt;00:00, 1.25kB/s]"
          }
        },
        "65a934267ff64c839b87861e0f173790": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "c3ee8f678b714794a097033d35406726": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "04f7d161a7844d249e82beaed6816e7e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "198c71e8566c41949e124eb0160ae7d4": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "1a6aeead7975454a8ce5f4ba5aa18ce1": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "e2d2b12d4f42496994d8ba6ed8525091": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "450c6b453eaf46448554b0f357a99746": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "43617af0c4bc471d8e77049e8450b374": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_a14161b08eab445388e66df210a1a103",
              "IPY_MODEL_7b90bfd485fb42928598c00c23f4dff6",
              "IPY_MODEL_c4f308054d7f4bb5b895c97feda36e59"
            ],
            "layout": "IPY_MODEL_e0611492f4f64e6196f3c772f1d8574b"
          }
        },
        "a14161b08eab445388e66df210a1a103": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_c14e10e2fd304215bb26cde981adb34a",
            "placeholder": "​",
            "style": "IPY_MODEL_de582300ab6b4620b03bf16aef94138c",
            "value": "config.json: 100%"
          }
        },
        "7b90bfd485fb42928598c00c23f4dff6": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_159d1ab01183470ba0c7458fe0340ec7",
            "max": 762,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_f6791bb357064008b751c9840092d617",
            "value": 762
          }
        },
        "c4f308054d7f4bb5b895c97feda36e59": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_1815b6025c9f48db9127a7ac19da9954",
            "placeholder": "​",
            "style": "IPY_MODEL_cf39e9cfb3c442d584d964fa6569b482",
            "value": " 762/762 [00:00&lt;00:00, 36.8kB/s]"
          }
        },
        "e0611492f4f64e6196f3c772f1d8574b": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "c14e10e2fd304215bb26cde981adb34a": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "de582300ab6b4620b03bf16aef94138c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "159d1ab01183470ba0c7458fe0340ec7": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "f6791bb357064008b751c9840092d617": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "1815b6025c9f48db9127a7ac19da9954": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "cf39e9cfb3c442d584d964fa6569b482": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "83c1b85be43d4b9d9588272e508d661b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_da60b228269f4eda96e758ead0d5bfc7",
              "IPY_MODEL_7fdfe523981f4c10ba6f0f9f57008442",
              "IPY_MODEL_da7020fb8a3d496696386541c2e89ae3"
            ],
            "layout": "IPY_MODEL_297aa3a8eae74d88b3b7697c54546a2b"
          }
        },
        "da60b228269f4eda96e758ead0d5bfc7": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_3cb9fa97989847eda93d482d33d78e17",
            "placeholder": "​",
            "style": "IPY_MODEL_45ccf917f9124d66b9a28154a3aebb43",
            "value": "vocab.json: 100%"
          }
        },
        "7fdfe523981f4c10ba6f0f9f57008442": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_d31ab45b38714fc58ba2a337477e3e6c",
            "max": 1042301,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_7d3ac4ecb9c54fc1a36304aeaa8b2f39",
            "value": 1042301
          }
        },
        "da7020fb8a3d496696386541c2e89ae3": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_50b515a9bf7b483e865b9eb0e52fd923",
            "placeholder": "​",
            "style": "IPY_MODEL_2793be2701194e56a792b80c977c9c05",
            "value": " 1.04M/1.04M [00:00&lt;00:00, 9.28MB/s]"
          }
        },
        "297aa3a8eae74d88b3b7697c54546a2b": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "3cb9fa97989847eda93d482d33d78e17": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "45ccf917f9124d66b9a28154a3aebb43": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "d31ab45b38714fc58ba2a337477e3e6c": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "7d3ac4ecb9c54fc1a36304aeaa8b2f39": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "50b515a9bf7b483e865b9eb0e52fd923": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "2793be2701194e56a792b80c977c9c05": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "583708bf57f143c6a8b643b7abbc4f28": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_b51300b3d8ef40e09f9ee10348fa44d5",
              "IPY_MODEL_2f8f062087234989bc1bd50243230835",
              "IPY_MODEL_50db6d96704347669b21a4de873b0335"
            ],
            "layout": "IPY_MODEL_8afe5892d2a445698573b5a9c900183e"
          }
        },
        "b51300b3d8ef40e09f9ee10348fa44d5": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_834cdc301eea4168a408a118094d4396",
            "placeholder": "​",
            "style": "IPY_MODEL_6ff9cfc34ab74f3fbf6cd6aa1ca2c811",
            "value": "merges.txt: 100%"
          }
        },
        "2f8f062087234989bc1bd50243230835": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_5c4e4e4577604ae7880005333a6891d1",
            "max": 456318,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_8529dc80f202467187129fb6dfb9fb47",
            "value": 456318
          }
        },
        "50db6d96704347669b21a4de873b0335": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_94fd597a1e8b4a7bb9012330c6144076",
            "placeholder": "​",
            "style": "IPY_MODEL_6a4a298e647d4617ad8c009f93809333",
            "value": " 456k/456k [00:00&lt;00:00, 6.46MB/s]"
          }
        },
        "8afe5892d2a445698573b5a9c900183e": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "834cdc301eea4168a408a118094d4396": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "6ff9cfc34ab74f3fbf6cd6aa1ca2c811": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "5c4e4e4577604ae7880005333a6891d1": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "8529dc80f202467187129fb6dfb9fb47": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "94fd597a1e8b4a7bb9012330c6144076": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "6a4a298e647d4617ad8c009f93809333": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "d42abe017596440188969f13c405764c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_fc87b68665df41908441a3f06784bcd2",
              "IPY_MODEL_256523dfc7f24566bf8ba8f4316ac372",
              "IPY_MODEL_32db5ac14d484349b282723e1eb2d719"
            ],
            "layout": "IPY_MODEL_8e58da124b794e0e95f5dac4c0e1ff5d"
          }
        },
        "fc87b68665df41908441a3f06784bcd2": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_180c11ee0dc04bc09b428b6ecfea1f18",
            "placeholder": "​",
            "style": "IPY_MODEL_9597b0611daf4bc6907ad41304600330",
            "value": "tokenizer.json: 100%"
          }
        },
        "256523dfc7f24566bf8ba8f4316ac372": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_57b009b604c7467d85fa6d6bd1b94b18",
            "max": 1355256,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_159af0890c0244579e7e992fcc951f7b",
            "value": 1355256
          }
        },
        "32db5ac14d484349b282723e1eb2d719": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_a733681a57bf41e184dc02bb287dc9af",
            "placeholder": "​",
            "style": "IPY_MODEL_3fe33242802b402fa4107b162a00bcbc",
            "value": " 1.36M/1.36M [00:00&lt;00:00, 9.61MB/s]"
          }
        },
        "8e58da124b794e0e95f5dac4c0e1ff5d": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "180c11ee0dc04bc09b428b6ecfea1f18": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "9597b0611daf4bc6907ad41304600330": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "57b009b604c7467d85fa6d6bd1b94b18": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "159af0890c0244579e7e992fcc951f7b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "a733681a57bf41e184dc02bb287dc9af": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "3fe33242802b402fa4107b162a00bcbc": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "40b8c13b9dbc41fd81e255c38dabc06a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_f8415178442942689f17c39c9d4e9079",
              "IPY_MODEL_be1bde7f97604894be30ada4e5439176",
              "IPY_MODEL_5ee8886188904061925cf57a47704fa8"
            ],
            "layout": "IPY_MODEL_2eccb868e7a947f1af4b77e6e712fb86"
          }
        },
        "f8415178442942689f17c39c9d4e9079": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_c1511faef52a4e938857e404c1584ecc",
            "placeholder": "​",
            "style": "IPY_MODEL_70f18b24d091421f806890bda8d82cca",
            "value": "model.safetensors: 100%"
          }
        },
        "be1bde7f97604894be30ada4e5439176": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_cb03b2c495c041648d14d95fd48a8757",
            "max": 352824413,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_8e2bb20a9e4a449e9e81f9bc16df177c",
            "value": 352824413
          }
        },
        "5ee8886188904061925cf57a47704fa8": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_e8918272a5f248558a66a2f7c6b569e5",
            "placeholder": "​",
            "style": "IPY_MODEL_63af51f3d0164a10b345e6d94ceb0c0b",
            "value": " 353M/353M [00:03&lt;00:00, 128MB/s]"
          }
        },
        "2eccb868e7a947f1af4b77e6e712fb86": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "c1511faef52a4e938857e404c1584ecc": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "70f18b24d091421f806890bda8d82cca": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "cb03b2c495c041648d14d95fd48a8757": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "8e2bb20a9e4a449e9e81f9bc16df177c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "e8918272a5f248558a66a2f7c6b569e5": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "63af51f3d0164a10b345e6d94ceb0c0b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "e40c16146e714ec6b2442ea55483195b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_1bc87d0420904b628d6f12a95a74292b",
              "IPY_MODEL_8478ed7604294b7293d1a8993b03a567",
              "IPY_MODEL_0307d6a18f414a08a2bac8a98a8fb31a"
            ],
            "layout": "IPY_MODEL_856dd14b0aac4e0f9ff682cee815bdb7"
          }
        },
        "1bc87d0420904b628d6f12a95a74292b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_d7f5cf0d1d87440ab4905adb890f1b15",
            "placeholder": "​",
            "style": "IPY_MODEL_e3bcf51c7e174ce2976eb71d35d23a13",
            "value": "generation_config.json: 100%"
          }
        },
        "8478ed7604294b7293d1a8993b03a567": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_bb3f7ac192dd4b8c8c829e7f262b330d",
            "max": 124,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_b90cb12f006e4dd08329c11b59b1ecad",
            "value": 124
          }
        },
        "0307d6a18f414a08a2bac8a98a8fb31a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_633d0632a85642a4ae108e078f2987ea",
            "placeholder": "​",
            "style": "IPY_MODEL_5b23a1f47a7e4838a1d6ac3c65ccd087",
            "value": " 124/124 [00:00&lt;00:00, 3.01kB/s]"
          }
        },
        "856dd14b0aac4e0f9ff682cee815bdb7": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "d7f5cf0d1d87440ab4905adb890f1b15": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "e3bcf51c7e174ce2976eb71d35d23a13": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "bb3f7ac192dd4b8c8c829e7f262b330d": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "b90cb12f006e4dd08329c11b59b1ecad": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "633d0632a85642a4ae108e078f2987ea": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "5b23a1f47a7e4838a1d6ac3c65ccd087": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}